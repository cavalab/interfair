{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969d6a34",
   "metadata": {},
   "source": [
    "# Mitigating Disparities\n",
    "\n",
    "This demo shows how to run the `mitigate_disparity` scripot on a development dataset. \n",
    "Below, we demonstrate how to run `mitgate_disparity.py` from the command line using a model trained to predict risk of admission to the emergency department using the freely available [MIMIC-IV repository](https://www.nature.com/articles/s41597-022-01899-x). \n",
    "\n",
    "## Inputs\n",
    "\n",
    "In addition to providing a dataset, the user should identify protected features by providing a list of column names corresponding to demographics and/or other variables over which fairness should be sought.\n",
    "\n",
    "## Continuous Updating\n",
    "\n",
    "This script may also be used to update a model with new data by passing a `starting_point` parameter. \n",
    "This allows models to be continously updated over time as new biases arise and dataset shift occurs, without having to start from scratch. \n",
    "Under the hood, this is done by setting `checkpoint=True` in the `FomoClassifier` object. \n",
    "See the [Fomo docs](https://cavalab.org/fomo/) for more information on options. \n",
    "\n",
    "You may also [browse the API](https://cavalab.org/interfair/api.html) for `mitigate_disparity.py`. \n",
    "\n",
    "\n",
    "Below, we run `mitigate_disparity.py` using a development dataset and specifying that we want to ensure fairness with respect to the features named ethnicity, gender, and insurance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919eea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../mitigate_disparity.py \\\n",
    "    --dataset ../data/mimic/development_dataset.train.csv \\\n",
    "    --protected_features ethnicity,gender,insurance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d6768",
   "metadata": {},
   "source": [
    "Calling `mitigate_disparity.py` will produce an `estimator.pkl` file that can be loaded for further analysis. \n",
    "We demonstrate this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5bbcb",
   "metadata": {},
   "source": [
    "## Visualize fairness/error trade-offs\n",
    "\n",
    "Once training is done, we can view a set of candidate models. \n",
    "The red dot indicates the model that was selected. \n",
    "In addition to the default \"PseudoWeights\" approach, FOMO provides other multi-criteria decsion making (MCDM) algorithms via pymoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccaf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../estimator.pkl','rb') as f:\n",
    "    est = pickle.load(f)\n",
    "est.plot().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f51dd7",
   "metadata": {},
   "source": [
    "## check test set performance\n",
    "\n",
    "This cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path to sys to import functions\n",
    "import os \n",
    "import sys\n",
    "dir_path = os.getcwd()\n",
    "sys.path.insert(0,os.path.abspath(os.path.join(dir_path, '..')))\n",
    "\n",
    "from utils import make_measure_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "with open('../estimator.pkl','rb') as f:\n",
    "    est = pickle.load(f)\n",
    "    \n",
    "df_test = pd.read_csv('../data/mimic/development_dataset.test.csv')\n",
    "X_test = df_test.drop(columns='binary outcome')\n",
    "y_test = df_test['binary outcome']\n",
    "make_measure_dataset(est, 'fomo', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a06a4f",
   "metadata": {},
   "source": [
    "## measure change in disparity measures\n",
    "\n",
    "Now that we have an updated model, we can check how our disparity measures have changed. \n",
    "Below we run `measure_disparity.py` with our new results and compare the results to the old ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from measure_disparity import measure_disparity\n",
    "measure_disparity('../fomo_model_mimic4_admission.csv', save_file='df_fairness.post.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efb1da",
   "metadata": {},
   "source": [
    "## Improvements over Baseline Model\n",
    "\n",
    "If we compare with results from our baseline model in [demo_measure_disparity.ipynb](https://cavalab.org/interfair/demo_measure_disparity.html), we see that we have made a marked improvement to the maximum subgroup deviations on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94faa3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(\n",
    "    tabulate(\n",
    "        [\n",
    "        [\"Max Subgroup Deviation in Metric (%)\",\"Original\",\"New\"],\n",
    "        [\"Brier Score (MSE)\",19.9, 19.3],\n",
    "        [\"Subgroup FNR\", 20.4, 10.9],\n",
    "        [\"Subgroup FPR\",86.0, 62.3],\n",
    "        [\"Positivity Rate\",44.9, 28.8],\n",
    "        ],\n",
    "        headers=\"firstrow\",\n",
    "        tablefmt='rounded_outline'\n",
    ")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "In summary, our new model has a more equal false negative rate among groups than before, which was our goal. \n",
    "In addition, we see reductions in the false positive rate deviations and differences in positivity rates. \n",
    "\n",
    "In terms of overall performance, we see a slight decrease, as we would also expect:\n",
    "\n",
    "- AUROC: 0.881 -> 0.859\n",
    "- AUPRC: 0.77 -> 0.74\n",
    "\n",
    "\n",
    "By using the model visualization tools above, decision makers can decide whether this model, or another within the set, is a better fit to the use case, as needed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
